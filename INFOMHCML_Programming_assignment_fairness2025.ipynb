{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvGd4YgtppjR"
      },
      "source": [
        "## Submission instructions\n",
        "\n",
        "All code that you write should be in this notebook. Please include your names and student numbers. You have to submit this notebook, with your code and answers filled in. Make sure to add enough documentation.\n",
        "\n",
        "For questions, make use of the \"Lab\" session (see schedule).\n",
        "Questions can also be posted to the MS teams channel called \"Lab\".\n",
        "\n",
        "**Note:** You are free to make use of Python libraries (e.g., numpy, sklearn, etc.) except any *fairness* libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P9dsbAffgfZ"
      },
      "source": [
        "#### Name and student numbers\n",
        "Jep Antonisse 3312070 Elias Hendriks 5930413 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ni3V-7iqA6X"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "In this assignment we are going to use the **COMPAS** dataset.\n",
        "\n",
        "If you haven't done so already, take a look at this article: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n",
        "For background on the dataset, see https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.\n",
        "\n",
        "**Reading in the COMPAS dataset**\n",
        "\n",
        "The dataset can be downloaded here: https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv\n",
        "\n",
        "For this assignment, we focus on the protected attribute *race*.\n",
        "\n",
        "The label (the variable we want to be able to predict) represents recidivism, which is defined as a new arrest within 2 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajLXEdx6plgP",
        "outputId": "a65bf36a-1bc6-488f-accc-7bd9b24ca411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-12 11:38:58--  https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -c https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AaT9DQwwpqkx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "compas_data = pd.read_csv('compas-scores-two-years.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsidUr4Bz-gZ"
      },
      "source": [
        "We apply several data preprocessing steps, including only retaining Caucasians and African Americans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vs2eRxdgrHrt"
      },
      "outputs": [],
      "source": [
        "compas_data = compas_data[(compas_data.days_b_screening_arrest <= 30)\n",
        "            & (compas_data.days_b_screening_arrest >= -30)\n",
        "            & (compas_data.is_recid != -1)\n",
        "            & (compas_data.c_charge_degree != 'O')\n",
        "            & (compas_data.score_text != 'N/A')\n",
        "            & ((compas_data.race == 'Caucasian') | (compas_data.race == 'African-American'))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5IwB6Rz2zIS"
      },
      "source": [
        "Take a look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMM-MYdstObf",
        "outputId": "0bf826a7-bf28-433b-ea6a-b21c5f3f1fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    id              name      first    last compas_screening_date     sex  \\\n",
            "1    3       kevon dixon      kevon   dixon            2013-01-27    Male   \n",
            "2    4          ed philo         ed   philo            2013-04-14    Male   \n",
            "6    8     edward riddle     edward  riddle            2014-02-19    Male   \n",
            "8   10  elizabeth thieme  elizabeth  thieme            2014-03-16  Female   \n",
            "10  14    benjamin franc   benjamin   franc            2013-11-26    Male   \n",
            "\n",
            "           dob  age       age_cat              race  ...  v_decile_score  \\\n",
            "1   1982-01-22   34       25 - 45  African-American  ...               1   \n",
            "2   1991-05-14   24  Less than 25  African-American  ...               3   \n",
            "6   1974-07-23   41       25 - 45         Caucasian  ...               2   \n",
            "8   1976-06-03   39       25 - 45         Caucasian  ...               1   \n",
            "10  1988-06-01   27       25 - 45         Caucasian  ...               4   \n",
            "\n",
            "    v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
            "1            Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
            "2            Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
            "6            Low        2014-02-19  2014-03-31   2014-04-18              14   \n",
            "8            Low        2014-03-16  2014-03-15   2014-03-18               0   \n",
            "10           Low        2013-11-26  2013-11-25   2013-11-26               0   \n",
            "\n",
            "   start  end event two_year_recid  \n",
            "1      9  159     1              1  \n",
            "2      0   63     0              1  \n",
            "6      5   40     1              1  \n",
            "8      2  747     0              0  \n",
            "10     0  857     0              0  \n",
            "\n",
            "[5 rows x 53 columns]\n"
          ]
        }
      ],
      "source": [
        "print(compas_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vKapT6FtmvJ"
      },
      "source": [
        "Now take a look at the distribution of the protected attribute `race` and the distribution of our outcome variable `two_year_recid`.\n",
        "\n",
        "**Note:** in the context of fair machine learning, the favorable label here is no recidivism, i.e., ```two_year_recid = 0```. So think about how what you will code as the positive class in your machine learning experiments, and make sure your interpretation of the results is consistent with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh2oM2yptnjR",
        "outputId": "f89ff857-41d9-4a8c-d236-6fd4af65f177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of instances per race category:\n",
            "race              two_year_recid\n",
            "African-American  1                 1661\n",
            "                  0                 1514\n",
            "Caucasian         0                 1281\n",
            "                  1                  822\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print('Number of instances per race category:')\n",
        "print(compas_data[['race', 'two_year_recid']].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636Yopp6wNtY"
      },
      "source": [
        "## Data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3g2NT98dY_"
      },
      "source": [
        "### **1. Exploration**\n",
        "\n",
        "First we perform an exploratory analysis of the data.\n",
        "\n",
        "**Question:** What is the size of the data? (i.e. how many data instances does it contain?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "k6FESAE1VmPu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of data instances:\n",
            "5278\n",
            "Number of features:\n",
            "53\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "print('Number of data instances:')\n",
        "print(compas_data.shape[0])\n",
        "print('Number of features:')\n",
        "print(compas_data.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx-EDiJ41-_O"
      },
      "source": [
        "**Question:** In the dataset, the protected attribute is `race`, which has two categories: White and African Americans. How many data instances belong to each category?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "e9WqGTz5237Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of instances per race category:\n",
            "race            \n",
            "African-American    3175\n",
            "Caucasian           2103\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "print('Number of instances per race category:')\n",
        "print(compas_data[['race']].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdMWSNK3OPt"
      },
      "source": [
        "**Question:** What are the base rates (the probability of a favorable outcome for the two protected attribute classes)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3OoqKyud3jIY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of individuals with no recidivism (two_year_recid == 0):\n",
            "African-American: 47.69%\n",
            "Caucasian: 60.91%\n",
            "----\n",
            "Percentage of individuals with no recidivism (two_year_recid == 0):\n",
            "Female: 63.82%\n",
            "Male: 50.32%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "def base_rate(attribute):\n",
        "    grouped_by_a = compas_data.groupby(attribute)['two_year_recid']\n",
        "    total_per_a = grouped_by_a.count()\n",
        "\n",
        "    no_recid = grouped_by_a.apply(lambda x: (x == 0).sum())\n",
        "    percentage_no_recid = (no_recid/total_per_a) * 100\n",
        "\n",
        "    percentages = []\n",
        "    print(\"Percentage of individuals with no recidivism (two_year_recid == 0):\")\n",
        "    for a in percentage_no_recid.index:\n",
        "        print(f\"{a}: {percentage_no_recid[a]:.2f}%\")\n",
        "        percentages.append(percentage_no_recid[a])\n",
        "    return percentages\n",
        "\n",
        "base_rate('race')\n",
        "print(\"----\")\n",
        "base_rate('sex')\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-0I7It3pBY"
      },
      "source": [
        "**Question:** What are the base rates for the combination of both race and sex categories?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3SCmOWs43t9r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of individuals with no recidivism (two_year_recid == 0):\n",
            "('African-American', 'Female'): 63.02%\n",
            "('African-American', 'Male'): 44.48%\n",
            "('Caucasian', 'Female'): 64.73%\n",
            "('Caucasian', 'Male'): 59.78%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "base_rate(['race', 'sex'])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qyyoS74UGC"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Write down a short interpretation of the statistics you calculated. What do you see?\n",
        "> Answer:\n",
        "- Most data instances (around 60%) have an African-American background\n",
        "- The percentage of recedivism is much lower for Caucasian people than African-American People\n",
        "- The percentage of recedivism is much lower for women than men. This holds for both races seperately as well\n",
        "- Only for African-American males is the recedivism percentage more than 50%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-mxqnnUcaGc"
      },
      "source": [
        "### **2. Performance measures**\n",
        "\n",
        "You will have to measure the performance and fairness of different classifiers in question 5. The performance will be calculated with the precision, recall, F1 and accuracy.\n",
        "Additionally, you will have to calculate the statistical/demographic parity, the true positive rate (recall) and false positive rate per race group.\n",
        "\n",
        "Make sure that you are able to calculate these metrics in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAO99gf2caZT"
      },
      "outputs": [],
      "source": [
        "# Your code for the performance measures\n",
        "def performance(model_predictions, true_labels_data):\n",
        "    # predictions have different indices, so we set them anew\n",
        "    model_predictions = model_predictions.reset_index(drop=True)\n",
        "    true_labels_data = true_labels_data.reset_index(drop=True)\n",
        "    \n",
        "    # Since no recidivism is the positive label, we can compute the confusion matrix:\n",
        "    tp = ((model_predictions['pred'] == 0) & (true_labels_data['two_year_recid'] == 0)).sum()\n",
        "    fp = ((model_predictions['pred'] == 0) & (true_labels_data['two_year_recid'] == 1)).sum()\n",
        "\n",
        "    tn = ((model_predictions['pred'] == 1) & (true_labels_data['two_year_recid'] == 1)).sum()\n",
        "    fn = ((model_predictions['pred'] == 1) & (true_labels_data['two_year_recid'] == 0)).sum()\n",
        "\n",
        "    # true positives / all positives\n",
        "    precision = tp / (tp + fp)\n",
        "\n",
        "    # true positives / all labels 0\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    # false positive / all labels 1\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    # all true predictions / total intences\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
        "    print(f\"Precision: {100 * precision:.2f}%\")\n",
        "    print(f\"Recall: {100 * recall:.2f}%\")\n",
        "    print(f\"False Negative Rate: {100 * fpr:.2f}\")\n",
        "    print(f\"F1-score: {f1:.2f}\")\n",
        "    return [accuracy, precision, recall, f1]\n",
        "\n",
        "def group_performance(model_predictions, data, true_labels_data, attribute):\n",
        "\n",
        "    data['pred'] = model_predictions['pred'].values #add the predictions to the datapoints\n",
        "    data['two_year_recid'] = true_labels_data['two_year_recid'].values #add the true lables to the datapoints\n",
        "\n",
        "    grouped_by_a = data.groupby(attribute)['pred'] #sort on \n",
        "    total_per_a = grouped_by_a.count()\n",
        "\n",
        "    no_recid = grouped_by_a.apply(lambda x: (x == 0).sum())\n",
        "    percentage_no_recid = (no_recid/total_per_a) * 100\n",
        "\n",
        "    print(\"Percentage of individuals in a subgroup with no recidivism predicted:\")\n",
        "    percentages = []\n",
        "    races = ['African-American', 'Caucasian']\n",
        "    for a in percentage_no_recid.index:\n",
        "        print(f\"{races[a]}: {percentage_no_recid[a]:.2f}%\")\n",
        "        percentages.append(percentage_no_recid[a])\n",
        "    print(f\"statistical parity: {percentages[0]/percentages[1]:.2f}\")\n",
        "\n",
        "    black = data[data['race'] == 0]\n",
        "    white = data[data['race'] == 1]\n",
        "\n",
        "    print(\"\\nDivided on sensitive attribute 'race':\")\n",
        "    print(\"\\nAfrican-American:\")\n",
        "    performance(black[['pred']], black[['two_year_recid']])\n",
        "    print(\"\\nCaucasian:\")\n",
        "    performance(white[['pred']], white[['two_year_recid']])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n51Bdpy59vhy"
      },
      "source": [
        "### **3. Prepare the data**\n",
        "For the classifiers in question 5, the input of the model can only contain numerical values, it is therefore important to convert the strings in the columns (features) of interest of the `compas_data` to floats or integers.\n",
        "\n",
        "The columns of interest are features that you think will be informative or interesting in predicting the outcome variable.\n",
        "Use the cell below to explore which of the Compas variables you need to convert to be able to use them for the classifiers.\n",
        "\n",
        "Generate a new dataframe with your selected features in the right encoding (also make sure to include `two_year_recid`). You can implement this yourself, or use the `LabelEncoder` from `sklearn`.\n",
        "\n",
        "**Note:** you do not need to convert all columns/features, only the ones you are interested in. However, do **not** include the feature `is_recid`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2G0-QxbH95rX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
            "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
            "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
            "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
            "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
            "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
            "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
            "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
            "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
            "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
            "       'decile_score.1', 'score_text', 'screening_date',\n",
            "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
            "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
            "       'start', 'end', 'event', 'two_year_recid'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>priors_count</th>\n",
              "      <th>c_charge_degree</th>\n",
              "      <th>r_charge_degree</th>\n",
              "      <th>vr_charge_degree</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>two_year_recid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    priors_count  c_charge_degree  r_charge_degree  vr_charge_degree  race  \\\n",
              "1              0                0                3                 2     0   \n",
              "2              4                0                6                 8     0   \n",
              "6             14                0                2                 8     1   \n",
              "8              0                1                9                 8     1   \n",
              "10             0                0                9                 8     1   \n",
              "\n",
              "    sex  two_year_recid  \n",
              "1     1               1  \n",
              "2     1               1  \n",
              "6     1               1  \n",
              "8     0               0  \n",
              "10    1               0  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code to prepare the data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "print(compas_data.columns)\n",
        "\n",
        "\n",
        "selected_features_categorical = ['c_charge_degree', 'r_charge_degree', 'vr_charge_degree']\n",
        "selected_features_numerical = ['priors_count']\n",
        "\n",
        "\n",
        "prepared_compas_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "# turn the strings into their exact integer value\n",
        "for feature in selected_features_numerical:\n",
        "    prepared_compas_data[feature] = compas_data[feature].astype(int)\n",
        "\n",
        "# for categorical features, we have to assign them integer values\n",
        "for feature in selected_features_categorical:\n",
        "    le = LabelEncoder()\n",
        "    prepared_compas_data[feature] = le.fit_transform(compas_data[feature])\n",
        "\n",
        "\n",
        "# Protected Features\n",
        "le = LabelEncoder()\n",
        "prepared_compas_data['race'] = le.fit_transform(compas_data['race'])\n",
        "le = LabelEncoder()\n",
        "prepared_compas_data['sex'] = le.fit_transform(compas_data['sex'])\n",
        "\n",
        "\n",
        "prepared_compas_data['two_year_recid'] = compas_data['two_year_recid'].astype(int)\n",
        "\n",
        "le = LabelEncoder()\n",
        "prepared_compas_data['race'] = le.fit_transform(compas_data['race'])\n",
        "le = LabelEncoder()\n",
        "prepared_compas_data['sex'] = le.fit_transform(compas_data['sex'])\n",
        "\n",
        "prepared_compas_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ILbSsR-ZW_"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Give a short motivation (one-two sentence) per feature why you think this is informative or interesting to take into account.\n",
        "> Answer:\n",
        "decile score: an algorithms score of a persons risks of recidivism. Since that is also our objective, this seems a usefull feature\n",
        "\n",
        "priors count: the number of offenses in the past can provide insight if the defendent has learned from previous mistakes. We purposely exclude felonies comitedded as a juvenile, since \n",
        "\n",
        "current charge degree: shows the extend of the current crime, on which we should base our verdict\n",
        "\n",
        "r/vr charge degree: gives some indication if defendent has already commited crimes on a higher level\n",
        "\n",
        "Personal information, such as age, were excluded after long consideration, since we deemed them irrelevant for the court. Law would and should treat adults similarly, despite an age difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgRPopIxNVJ"
      },
      "source": [
        "### **4. Train and test split**\n",
        "\n",
        "Divide the dataset into a train (80%) and test split (20%), either by implementing it yourself, or by using an existing library.\n",
        "\n",
        "**Note:** Usually when carrying out machine learning experiments,\n",
        "we also need a dev set for developing and selecting our models (incl. tuning of hyper-parameters).\n",
        "However, in this assignment, the goal is not to optimize\n",
        "the performance of models so we'll only use a train and test split.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "G0wUGEpiV7mH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>priors_count</th>\n",
              "      <th>c_charge_degree</th>\n",
              "      <th>r_charge_degree</th>\n",
              "      <th>vr_charge_degree</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6187</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3989</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      priors_count  c_charge_degree  r_charge_degree  vr_charge_degree  race  \\\n",
              "3811             0                0                9                 8     0   \n",
              "6187            11                0                7                 8     0   \n",
              "1491             7                0                9                 8     0   \n",
              "3989             0                0                9                 8     0   \n",
              "4740             3                1                9                 8     1   \n",
              "\n",
              "      sex  \n",
              "3811    1  \n",
              "6187    1  \n",
              "1491    1  \n",
              "3989    1  \n",
              "4740    1  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code to split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    prepared_compas_data[selected_features_numerical + selected_features_categorical + ['race', 'sex']], prepared_compas_data['two_year_recid'], test_size=0.2  # 20% test, 80% train\n",
        ")\n",
        "\n",
        "x_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmq45GgAWEJo"
      },
      "source": [
        "### **5. Classifiers**\n",
        "\n",
        "Now, train and test different classifiers and report the following statistics:\n",
        "\n",
        "* Overall performance:\n",
        "\n",
        "  * Precision\n",
        "  * Recall\n",
        "  * F1\n",
        "  * Accuracy\n",
        "\n",
        "* Fairness performance:\n",
        "\n",
        "  * The statistical parity difference for the protected attribute `race`(i.e. the difference in the probability of receiving a favorable label between the two protected attribute groups);\n",
        "  * The true positive rates of the two protected attribute groups\n",
        "  * The false positive rates of the two protected attribute groups.\n",
        "\n",
        "For training the classifier we recommend using scikit-learn (https://scikit-learn.org/stable/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu6eQ_3xGfXd"
      },
      "source": [
        "#### **5.1 Regular classification**\n",
        "Train a logistic regression classifier with the race feature and all other features that you are interested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'model_predictions' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m predictions = pd.DataFrame(model_with_race.predict(x_test[selected_features_numerical + selected_features_categorical + [\u001b[33m'\u001b[39m\u001b[33mrace\u001b[39m\u001b[33m'\u001b[39m]]), columns=[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m true_labels = pd.DataFrame(y_test, columns=[\u001b[33m'\u001b[39m\u001b[33mtwo_year_recid\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m perf = \u001b[43mperformance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m group_performance(predictions, x_test, true_labels, \u001b[33m'\u001b[39m\u001b[33mrace\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mperformance\u001b[39m\u001b[34m(predictions, true_labels)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mperformance\u001b[39m(predictions, true_labels):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# predictions have different indices, so we set them anew\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model_predictions = \u001b[43mmodel_predictions\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     true_labels_data = true_labels_data.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Since no recidivism is the positive label, we can compute the confusion matrix:\u001b[39;00m\n",
            "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'model_predictions' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# Your code for classifier 1\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_with_race = LogisticRegression()\n",
        "model_with_race.fit(x_train[selected_features_numerical + selected_features_categorical + ['race']], y_train)\n",
        "\n",
        "predictions = pd.DataFrame(model_with_race.predict(x_test[selected_features_numerical + selected_features_categorical + ['race']]), columns=['pred'])\n",
        "true_labels = pd.DataFrame(y_test, columns=['two_year_recid'])\n",
        "\n",
        "perf = performance(predictions, true_labels)\n",
        "group_performance(predictions, x_test, true_labels, 'race')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **5.2 Without the protected attribute**\n",
        "Train a logistic regression classifier without the race feature, but with all other features you used in 5.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL7LSSMPFmhv"
      },
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'model_predictions' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m wr_predictions = pd.DataFrame(model_without_race.predict(x_test[selected_features_categorical + selected_features_numerical]), columns=[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m wr_true_labels = pd.DataFrame(y_test, columns=[\u001b[33m'\u001b[39m\u001b[33mtwo_year_recid\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m wr_perf = \u001b[43mperformance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwr_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwr_true_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m group_performance(wr_predictions, x_test, wr_true_labels, \u001b[33m'\u001b[39m\u001b[33mrace\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mperformance\u001b[39m\u001b[34m(predictions, true_labels)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mperformance\u001b[39m(predictions, true_labels):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# predictions have different indices, so we set them anew\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model_predictions = \u001b[43mmodel_predictions\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     true_labels_data = true_labels_data.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Since no recidivism is the positive label, we can compute the confusion matrix:\u001b[39;00m\n",
            "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'model_predictions' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# Your code for classifier 2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_without_race = LogisticRegression()\n",
        "model_without_race.fit(x_train[selected_features_categorical + selected_features_numerical], y_train)\n",
        "\n",
        "wr_predictions = pd.DataFrame(model_without_race.predict(x_test[selected_features_categorical + selected_features_numerical]), columns=['pred'])\n",
        "wr_true_labels = pd.DataFrame(y_test, columns=['two_year_recid'])\n",
        "\n",
        "wr_perf = performance(wr_predictions, wr_true_labels)\n",
        "group_performance(wr_predictions, x_test, wr_true_labels, 'race')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H_mlbM6qij9"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Write down a short interpretation of the results you calculated. What do you see?\n",
        "> Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwiX0vEXFlgU"
      },
      "source": [
        "#### **5.3 Pre-processing: Reweighing**\n",
        "Train and test a classifier with weights (see lecture slide for the weight calculation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-1Vw1_xk4aQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If race and label are independent, we would expect to observe the probabilies:\n",
            "African-American, 0: 0.3186\n",
            "African-American, 1: 0.2830\n",
            "Caucasian, 0: 0.2110\n",
            "Caucasian, 1: 0.1874\n",
            "\n",
            "Therefore, we need to reweigh into:\n",
            "African-American, 0: 1.1105\n",
            "African-American, 1: 0.8993\n",
            "Caucasian, 0: 0.8694\n",
            "Caucasian, 1: 1.2036\n",
            "\n",
            "Accuracy: 97.63%\n",
            "Precision: 99.25%\n",
            "Recall: 96.17%\n",
            "F1-score: 0.98\n",
            "Percentage of individuals in a subgroup with no recidivism predicted:\n",
            "African-American: 45.66%\n",
            "Caucasian: 57.14%\n",
            "statistical parity: 0.80\n",
            "\n",
            "Divided on sensitive attribute 'race':\n",
            "\n",
            "African-American:\n",
            "Accuracy: 97.24%\n",
            "Precision: 99.60%\n",
            "Recall: 95.74%\n",
            "F1-score: 0.98\n",
            "\n",
            "Caucasian:\n",
            "Accuracy: 97.91%\n",
            "Precision: 98.94%\n",
            "Recall: 96.56%\n",
            "F1-score: 0.98\n"
          ]
        }
      ],
      "source": [
        "# Your code for classifier 3\n",
        "def base_rate_reweigh(attribute):\n",
        "    # Group sizes probability:\n",
        "    group_size = compas_data[[attribute]].value_counts()\n",
        "    group_prob = group_size/len(compas_data)\n",
        "\n",
        "    # Label probability\n",
        "    label_size = compas_data[['two_year_recid']].value_counts()\n",
        "    label_prob = label_size/len(compas_data)\n",
        "\n",
        "    print(f\"If {attribute} and label are independent, we would expect to observe the probabilies:\")\n",
        "    for a in group_prob.index:\n",
        "        for l in label_prob.index:\n",
        "            print(f\"{a[0]}, {l[0]}: {group_prob[a] * label_prob[l]:.4f}\")\n",
        "\n",
        "    #print(\"\\nHowever we observe:\")\n",
        "    observed_probability = compas_data[['race', 'two_year_recid']].value_counts() / len(compas_data)\n",
        "    #print(observed_probability)\n",
        "\n",
        "    print(\"\\nTherefore, we need to reweigh into:\")\n",
        "    for a in group_prob.index:\n",
        "        for l in label_prob.index:\n",
        "            print(f\"{a[0]}, {l[0]}: {((group_prob[a] * label_prob[l]) / observed_probability[a[0], l[0]]):.4f}\")\n",
        "    return\n",
        "\n",
        "\n",
        "base_rate_reweigh('race')\n",
        "\n",
        "print()\n",
        "\n",
        "weights = []\n",
        "\n",
        "for index, point in x_train.iterrows():\n",
        "    race = point['race']\n",
        "    recid = y_train.loc[index]\n",
        "    \n",
        "    if race == 0 and recid == 0:\n",
        "        weights.append(1.1105)\n",
        "    elif race == 0 and recid == 1:\n",
        "        weights.append(0.8993)\n",
        "    elif race == 1 and recid == 0:\n",
        "        weights.append(0.8694)\n",
        "    elif race == 1 and recid == 1:\n",
        "        weights.append(1.2036)\n",
        "\n",
        "weighted_model_without_race = LogisticRegression()\n",
        "weighted_model_without_race.fit(x_train[selected_features_categorical + selected_features_numerical], y_train,\n",
        "sample_weight=weights)\n",
        "\n",
        "rw_predictions = pd.DataFrame(weighted_model_without_race.predict(x_test[selected_features_categorical + selected_features_numerical]), columns=['pred'])\n",
        "rw_true_labels = pd.DataFrame(y_test, columns=['two_year_recid'])\n",
        "\n",
        "\n",
        "rw_perf = performance(rw_predictions, rw_true_labels)\n",
        "group_performance(rw_predictions, x_test, rw_true_labels, 'race')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3XdW0qF82JC"
      },
      "source": [
        "#### **5.4 Post-processing: Equalized odds**\n",
        "Use the predictions by the first classifier for this post processing part (see lecture slides for more information about post processing for equalized odds).\n",
        "\n",
        "We have the following parameters (A indicates group membership, Y_{hat} the original prediction, Y_{tilde} the prediction of the derived predictor).\n",
        "\n",
        "* `p_00` = P(Y_{tilde} = 1 | Y_{hat} = 0 & A = 0)\n",
        "* `p_01` = P(Y_{tilde} = 1 | Y_{hat} = 0 & A = 1)\n",
        "* `p_10` = P(Y_{tilde} = 1 | Y_{hat} = 1 & A = 0)\n",
        "* `p_11` = P(Y_{tilde} = 1 | Y_{hat} = 1 & A = 1)\n",
        "\n",
        "\n",
        "Normally, the best parameters `p_00, p_01, p_10, p_11` are found with a linear program that minimizes loss between predictions of a derived predictor and the actual labels. In this assignment we will not ask you to do this. Instead, we would like you to follow the next steps to find parameters, post-process the data and check the performance of this classifier with post-processing:\n",
        "\n",
        "1. Generate 5000 different samples of these 4 parameters randomly;\n",
        "2. Write a function (or more) that applies these 4 parameters to postprocess the predictions.\n",
        "3. For each generated set of 4 parameters:\n",
        "  - Change the predicted labels with the function(s) from step 2;\n",
        "  - Evaluate these 'new' predictions, by calculating group-wise TPR and FPR, as well as overall performance based on F1 and/or accuracy.\n",
        "4. Choose the best set of parameters. Take into account the equalized odds fairness measure, as well a performance measure like accuracy or F1.\n",
        "5. Check the overall performance (precision, recall, accuracy, F1, etc.) of the new predictions after post-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yHv79yvv00a"
      },
      "source": [
        "**Question**\n",
        "\n",
        " Report the 4 weights that are used for reweighing and a short **interpretation/discussion** of the weights and the classifier results.\n",
        "> Answer:\n",
        "The weights for combinations of race and outcome:\n",
        "African-American, 0: 1.1105,\n",
        "African-American, 1: 0.8993,\n",
        "Caucasian, 0: 0.8694,\n",
        "Caucasian, 1: 1.2036,\n",
        "\n",
        "This shows that African-American in combination with 0 and Caucasian in combination with 1 have smaller probabilities than expected with an independent relation between attribute and outcome. Therefore, we increase their weight in the classifier slightly.\n",
        "\n",
        "However, as far as we can see, it has no effect on the final result of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk_scQdM76He"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(0, 0): 0.4790656587364649, (0, 1): 0.8248352384881625, (1, 0): 0.9724002898154778, (1, 1): 0.8238759748477753}\n"
          ]
        }
      ],
      "source": [
        "# Your code for step 1\n",
        "import random\n",
        "\n",
        "random_parameters = []\n",
        "for _ in range(5000):\n",
        "  p_00 = random.uniform(0,1)\n",
        "  p_01 = random.uniform(0,1)\n",
        "  p_10 = random.uniform(0,1)\n",
        "  p_11 = random.uniform(0,1)\n",
        "  random_parameters.append({(0, 0): p_00,\n",
        "                            (0, 1): p_01,\n",
        "                            (1, 0): p_10,\n",
        "                            (1, 1): p_11})\n",
        "\n",
        "# Example, first set of random parameters\n",
        "print(random_parameters[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0p92txObr6v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters:\n",
            "  P(Ỹ=1 | Ŷ=0, A=0) = 0.091\n",
            "  P(Ỹ=1 | Ŷ=0, A=1) = 0.183\n",
            "  P(Ỹ=1 | Ŷ=1, A=0) = 0.914\n",
            "  P(Ỹ=1 | Ŷ=1, A=1) = 0.972\n",
            "\n",
            "Group-wise TPR/FPR:\n",
            "  Group 0: TPR = 0.960, FPR = 0.136\n",
            "  Group 1: TPR = 0.970, FPR = 0.192\n",
            "\n",
            "Overall performance:\n",
            "Accuracy: 89.77%\n",
            "Precision: 84.34%\n",
            "Recall: 96.65%\n",
            "F1-score: 0.90\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Your code for step 2\n",
        "# Create a dataframe with the necessary information\n",
        "compas_data = compas_data[compas_data['race'].isin(['African-American', 'Caucasian'])].copy()\n",
        "compas_data['race_num'] = compas_data['race'].map({'Caucasian': 0, 'African-American': 1})\n",
        "\n",
        "pred_labels_df = predictions.values.ravel()\n",
        "true_labels_df = true_labels.values.ravel()\n",
        "compas_subset = compas_data.loc[x_test.index].copy()\n",
        "\n",
        "df_post_data = pd.DataFrame({'race_num': compas_subset['race_num'].values, #Deze krijg ik niet de goede size\n",
        "                             'pred_labels': pred_labels_df,\n",
        "                             'true_labels': true_labels_df})\n",
        "\n",
        "# the number of cases falling in each condition\n",
        "subset_sizes = {\n",
        "    (0, 0): len(df_post_data.query('pred_labels == 0 & race_num == 0')),\n",
        "    (0, 1): len(df_post_data.query('pred_labels == 0 & race_num == 1')),\n",
        "    (1, 0): len(df_post_data.query('pred_labels == 1 & race_num == 0')),\n",
        "    (1, 1): len(df_post_data.query('pred_labels == 1 & race_num == 1'))\n",
        "}\n",
        "\n",
        "def generate_labels(subset_sizes, p_dict):\n",
        "    new_predictions = {}\n",
        "\n",
        "    for (prediction, group), p in p_dict.items():\n",
        "\n",
        "      # The number of instances for which we need to generate labels\n",
        "      num_instances = subset_sizes[(prediction, group)]\n",
        "\n",
        "      # Write your code here.\n",
        "      new_labels = np.random.binomial(n=1, p=p, size=num_instances)\n",
        "      # save the new predictions\n",
        "      new_predictions[(prediction, group)] = new_labels\n",
        "\n",
        "    return new_predictions\n",
        "  \n",
        "def post_processing(df, new_predictions):\n",
        "    df = df.copy()\n",
        "    df['postprocessed_preds'] = np.nan\n",
        "\n",
        "    for (pred_val, group_val), labels in new_predictions.items():\n",
        "        mask = (df['pred_labels'] == pred_val) & (df['race_num'] == group_val)\n",
        "        df.loc[mask, 'postprocessed_preds'] = labels\n",
        "\n",
        "    df['postprocessed_preds'] = df['postprocessed_preds'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Compute TPR/FPR per group and overall metrics\n",
        "def compute_group_metrics(df, pred_col='postprocessed_preds'):\n",
        "    group_stats = {}\n",
        "    for group in [0, 1]:\n",
        "        sub = df[df['race_num'] == group]\n",
        "        y_true = sub['true_labels']\n",
        "        y_pred = sub[pred_col]\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        group_stats[group] = {'TPR': tpr, 'FPR': fpr}\n",
        "    return group_stats\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "teZWyupiw2iM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 3\u001b[39;00m\n\u001b[32m      2\u001b[39m best_result = {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m: -\u001b[43mnp\u001b[49m.inf,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgroup_metrics\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p_set \u001b[38;5;129;01min\u001b[39;00m random_parameters:\n\u001b[32m     10\u001b[39m     new_preds = generate_labels(subset_sizes, p_set)\n",
            "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 3\n",
        "best_result = {\n",
        "    'score': -np.inf,\n",
        "    'params': None,\n",
        "    'df': None,\n",
        "    'group_metrics': None\n",
        "}\n",
        "\n",
        "for p_set in random_parameters:\n",
        "    new_preds = generate_labels(subset_sizes, p_set)\n",
        "    df_eval = post_processing(df_post_data, new_preds)\n",
        "    group_metrics = compute_group_metrics(df_eval)\n",
        "\n",
        "    # Calculate TPR gap and F1\n",
        "    tpr_gap = abs(group_metrics[0]['TPR'] - group_metrics[1]['TPR'])\n",
        "\n",
        "    y_true = df_eval['true_labels'].values\n",
        "    y_pred = df_eval['postprocessed_preds'].values\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    score = f1 - tpr_gap\n",
        "\n",
        "    if score > best_result['score']:\n",
        "        best_result.update({\n",
        "            'score': score,\n",
        "            'params': p_set,\n",
        "            'df': df_eval,\n",
        "            'group_metrics': group_metrics,\n",
        "            'confusion_matrix': (tn, fp, fn, tp)\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q-t-ZljWBBS9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'best_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Your code for step 4 and 5\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Step 4\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (yhat, group), prob \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbest_result\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m].items():\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  P(Ỹ=1 | Ŷ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myhat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, A=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGroup-wise TPR/FPR:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'best_result' is not defined"
          ]
        }
      ],
      "source": [
        "# Your code for step 4 and 5\n",
        "# Step 4\n",
        "print(\"Best parameters:\")\n",
        "for (yhat, group), prob in best_result['params'].items():\n",
        "    print(f\"  P(Ỹ=1 | Ŷ={yhat}, A={group}) = {prob:.3f}\")\n",
        "\n",
        "print(\"\\nGroup-wise TPR/FPR:\")\n",
        "for g, stats in best_result['group_metrics'].items():\n",
        "    print(f\"  Group {g}: TPR = {stats['TPR']:.3f}, FPR = {stats['FPR']:.3f}\")\n",
        "\n",
        "# Step 5\n",
        "tn, fp, fn, tp = best_result['confusion_matrix']\n",
        "\n",
        "precision = tp / (tp + fp) if (tp + fp) else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) else 0\n",
        "accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "print(\"\\nOverall performance:\")\n",
        "print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
        "print(f\"Precision: {100 * precision:.2f}%\")\n",
        "print(f\"Recall: {100 * recall:.2f}%\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD0weMlkN4Cq"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Describe how you selected the best set of parameters. Furthermore, how do you interpret the best set of parameters that you found? And what do you think of the results of the new classifier?\n",
        ">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwtRtf1Pl5_u"
      },
      "source": [
        "#### **Overall discussion**\n",
        "For all 4 classifiers that you trained, describe:\n",
        "- Does this classifier satisfies statistical parity?\n",
        "- Does the classifier satisfy the equal opportunity criterion?\n",
        "\n",
        "Finally, how do the different classifiers compare against each other?\n",
        "\n",
        ">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUPZN32kOJ9H"
      },
      "source": [
        "### **6. Intersectional fairness**\n",
        "In the questions above `race` was the only protected attribute. However, multiple protected attributes sometimes interact, leading to different fairness outcomes for different combinations of these protected attributes.\n",
        "\n",
        "Now explore the intersectional fairness for protected attributes `race` and `sex` for the first two classifiers from question 5. Make a combination of the `race` and `sex` column, resulting in four new subgroups (e.g., female Caucasian), and report the maximum difference between the subgroups for statistical parity, TPR and FPR.\n",
        "For example, suppose we have four groups with TPRs 0.1, 0.2, 0.3, 0.8, then the maximum difference is 0.7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-siRd0cUH0sN"
      },
      "source": [
        "Your code to evaluate intersectional fairness for Classifier 1:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSXG9sBjT-xX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intersectional statistical parity with no recidivism predicted\n",
            "1_Male: 0.438\n",
            "0_Female: 0.606\n",
            "0_Male: 0.561\n",
            "1_Female: 0.538\n",
            "1_Male: 0.989\n",
            "0_Female: 0.976\n",
            "0_Male: 1.000\n",
            "1_Female: 1.000\n",
            "1_Male: 0.044\n",
            "0_Female: 0.016\n",
            "0_Male: 0.051\n",
            "1_Female: 0.000\n",
            "Max disparity:\n",
            "Statistical Parity gap: 0.1681454683929931\n",
            "TPR gap: 0.024390243902439046\n",
            "FPR gap: 0.05128205128205128\n"
          ]
        }
      ],
      "source": [
        "# Your code for intersectional fairness\n",
        "compas_subset = compas_data.loc[x_test.index].copy()\n",
        "df_post_data['sex'] = compas_subset['sex'].values\n",
        "\n",
        "df_post_data['intersectional'] = df_post_data['race_num'].astype(str) + \"_\" + df_post_data['sex'].astype(str)\n",
        "\n",
        "parity_intersect = {}\n",
        "TPR_intersect = {}\n",
        "FPR_intersect = {}\n",
        "\n",
        "for combo in df_post_data['intersectional'].unique():\n",
        "    sectional = df_post_data[df_post_data['intersectional'] == combo]\n",
        "    parity_intersect[combo] = (sectional['pred_labels'] == 0).mean()\n",
        "    \n",
        "    y_true = sectional['true_labels']\n",
        "    y_pred = sectional['pred_labels']\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    TPR_intersect[combo] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    FPR_intersect[combo] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "print(\"Intersectional statistical parity with no recidivism predicted\")\n",
        "for group, value in parity_intersect.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "for group, value in TPR_intersect.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "for group, value in FPR_intersect.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "    \n",
        "print()\n",
        "print(\"Max disparity:\")\n",
        "print(f\"Statistical Parity gap: {max(parity_intersect.values()) - min(parity_intersect.values())}\")\n",
        "print(f\"TPR gap: {max(TPR_intersect.values()) - min(TPR_intersect.values())}\")\n",
        "print(f\"FPR gap: {max(FPR_intersect.values()) - min(FPR_intersect.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz_hNpkGH1ul"
      },
      "source": [
        "Your code to evaluate intersectional fairness for Classifier 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwvWChS2H79s"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'wr_predictions' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Your code for intersectional fairness Classifier 2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pred_labels_2 = \u001b[43mwr_predictions\u001b[49m.values.ravel()\n\u001b[32m      4\u001b[39m true_labels_2 = wr_true_labels.values.ravel()\n\u001b[32m      5\u001b[39m compas_subset_2 = compas_data.loc[x_test.index].copy()\n",
            "\u001b[31mNameError\u001b[39m: name 'wr_predictions' is not defined"
          ]
        }
      ],
      "source": [
        "# Your code for intersectional fairness Classifier 2\n",
        "\n",
        "pred_labels_2 = wr_predictions.values.ravel()\n",
        "true_labels_2 = wr_true_labels.values.ravel()\n",
        "compas_subset_2 = compas_data.loc[x_test.index].copy()\n",
        "\n",
        "df_post_data_2 = pd.DataFrame({'race_num': compas_subset_2['race_num'].values,\n",
        "                             'pred_labels': pred_labels_2,\n",
        "                             'true_labels': true_labels_2})\n",
        "compas_subset_2 = compas_data.loc[x_test.index].copy()\n",
        "df_post_data_2['sex'] = compas_subset_2['sex'].values\n",
        "\n",
        "df_post_data_2['intersectional'] = df_post_data_2['race_num'].astype(str) + \"_\" + df_post_data_2['sex'].astype(str)\n",
        "\n",
        "parity_intersect_2 = {}\n",
        "TPR_intersect_2 = {}\n",
        "FPR_intersect_2 = {}\n",
        "\n",
        "for combo in df_post_data_2['intersectional'].unique():\n",
        "    sectional_2 = df_post_data_2[df_post_data_2['intersectional'] == combo]\n",
        "    parity_intersect_2[combo] = (sectional['pred_labels'] == 0).mean()\n",
        "    \n",
        "    y_true_2 = sectional_2['true_labels']\n",
        "    y_pred_2 = sectional_2['pred_labels']\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_true_2, y_pred_2, labels=[0, 1]).ravel()\n",
        "    TPR_intersect_2[combo] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    FPR_intersect_2[combo] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "print(\"Intersectional statistical parity with no recidivism predicted\")\n",
        "for group, value in parity_intersect_2.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "for group, value in TPR_intersect_2.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "for group, value in FPR_intersect_2.items():\n",
        "    print(f\"{group}: {value:.3f}\")\n",
        "    \n",
        "print()\n",
        "print(\"Max disparity:\")\n",
        "print(f\"Statistical Parity gap: {max(parity_intersect_2.values()) - min(parity_intersect_2.values())}\")\n",
        "print(f\"TPR gap: {max(TPR_intersect_2.values()) - min(TPR_intersect_2.values())}\")\n",
        "print(f\"FPR gap: {max(FPR_intersect_2.values()) - min(FPR_intersect_2.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyE6wo8EH-E0"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Write down a short interpretation of the results you calculated. What do you see?\n",
        "> Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lfFYnU1V_bl"
      },
      "source": [
        "## Discussion\n",
        "Provide a short ethical discussion (1 or 2 paragraphs) reflecting on these two aspects:\n",
        "\n",
        "1) The use of a ML system to try to predict recidivism;\n",
        "\n",
        "2) The public release of a dataset like this.\n",
        "\n",
        "> Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEjSKr56xiZO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
